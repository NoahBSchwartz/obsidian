*- Goal: Apply rule-based reasoning to natural language*
*- Innovation: NLProlog is an extension of Neural Theorem Provers (prolog systems tailored for question-answering in NLP). It improves their scalability by cutting out the lowest ranked branches the program could take. And it adds weak unification which creates a reasoner that can use Soft Logic (gradient from 0 to 1) in not only symbol comparisons (which the NTP pioneered) but also logic expression comparisons. Finally, it uses pretrained encoders which makes the system much better.*
### Background
- Multi-Hop Reasoning: answer requires drawing inferences from multiple linked (but not directly connected) pieces of information (ex. If X = Y and Y = Z then X = Z)
	- Deep Learning Models: 
		1. Pros: It can deal well with ambiguity of language
		2. Cons: Hard to explain steps a model took to get to an answer. 
		     Hard to use the model parameters to extrapolate new knowledge.
		     Requires large amounts of training data.
		     Sometimes counter-intuitive results.
		     Difficult to guide the models to capture desired patterns. 
		- Past Approaches (trying to eliminate cons)
			- The question is updated by adding on relevant information from the source. Then an RNN is used to focus on relevant parts sequentially (recurrent LLM = processing text in sequence)
			- Differentiable Memory Structure: (Add)
			- Coreference Information: (Add)
			- Graph Convolutional Networks: (Add)
	- Rule-Based Models: 
		1. Pros: easily interpretable, naturally produces explanations for decisions, can generalize from smaller amounts of data.
		2. Cons: doesn't deal with noise well. Can't deal with ambiguity of language or vision
		- Past Approaches (trying to eliminate cons)
			- Markov Logic Networks: prolog reasoners that use probabilistic models as stand ins for symbol comparison 
			- Problems: MLN's still use binary logic when comparing ideas together instead of soft logic (gradient from 0 to 1)
	- Neuro-Symbolic Models:
		- Past Approaches (General)
			- Knowledge Base Completion: the process of automatically adding new information to a knowledge base (structured database of facts). When organized in the form 
					entities      |       attributes      |      relationships
			   it can act as another way to preform multi-hop reasoning. But this is incredibly hard to do with language
				- Neural Theorem Provers: A type of prolog solver that can work with knowledge bases by employing a neural net to make soft comparisons (giving a gradient from 0 to 1). However, NTPs can't apply to large knowledge bases so they can't be used for NLP.
		- Past Approaches (Specific to Question Answering)
			- Watson: transforms natural language sentences to logical atoms and then uses a theorem prover
### NLProlog General Idea
Combine a symbolic reasoner and a rule-learning method
- The authors use a simple prolog reasoner but replace the symbols for concepts. These concepts are represented in a higher dimensional space (word embeddings). The similarity between word embeddings is used by the prolog reasoner as a stand in for comparing symbols (during unification)
	- Embeddings (Distributed Representations): represent words as vectors in a high-dimensional space
	- Encoder: transforms raw text into vectors (in DNN's, these are end-to-end differentiable meaning the function is continuous to allow for gradient descent)
	- Pretrained Embeddings: the authors start with vectors generated by a model which has already been trained on a lot of text 
	- Finetuning: the authors then finetune the model to better handle the specifics of a given domain (Add)
### NLProlog Steps
1. Preprocessing: extract "triples" from the text (entity A, a relation that connects A -> B,  entity B). Ex. Sentence: "Barack Obama was born in Hawaii" Triple: Barack Obama, born in, Hawaii 
		- Note: the connecting relation (ie. born in) is called the textual surface pattern
	- Focusses on where words occur in proximity to each other within sentences to figure out relationships (reduces complexity)
	1. Identify distinct objects (ie. names, places, things) in the text using SPACY
	2. Take 2 objects that appear in the same sentence and use the entire sentence as a connector between them
	3. Construct triple `(object 1, connector, object 2)`
2. Neural Setup: 
	- Embedding: embed each element of the triple as a vector in high-dimensional space (the authors use pretrained models for this)
		1. Encode Objects: using a randomly initialized embedding matrix  
		2. Encode Relations for Facts: using a pretrained sentence encoder (SENT2VEC) with an MLP tacked on (for finetuning)
		3. Encode Relations for Rules and Queries: using a concrete lookup table with an MLP tacked on (for finetuning)
		 Note: #3 is needed because relations can have multiple meaning when used in rules or queries 
	- The vectors are used by the similarity function for computing similarities between 2 entities or 2 relations
3. Prolog Setup:
	- Prologs consist of many individual rules (one is pictured below) all stored in one database. Every rule has a head, the thing you can deduce if the conditions specified in the body are met. The body may contain one or more conditions, and all of these conditions must be true for the head of the clause to be true.
		- Functions (f): specific entities or values (ie. person('Jack'), city('New York')). The domain function symbols is the set of all possible objects that function symbols can represent
		- Variables (F): placeholders that can represent any entity (ie. X, Y)
		- Predicate Symbols (h, p): relations or actions (ie. parent, is_a, larger_than, likes). The domain of predicate symbols is the set of all actions that can be represented in the program
		- Rule:![[Screenshot 2024-01-01 at 10.32.11 PM 3.png]]
	 - Atomic Formula: rules which don't depend on any other rules to be true (ie. born(Name, Day, Month, Year))
		- Fact: when the rule doesn't contain any variables![[Screenshot 2024-01-01 at 10.41.44 PM.png]]
		- Query![[Screenshot 2024-01-01 at 10.41.51 PM.png]]
	- *Note: the authors choose to only use functions with an arity of 0 meaning that an object like person(Name) is used in their approach but not anything more complex like date(Day, Month, Year)
4. Prolog Run-time: 
	- Weak Unification (fuzzy): Determines whether a rule can be applied to a given situation by checking if the rule's conditions can be made to match the current state (ie. can the variables in 1 expression be substituted so that its equivalent to another expression). Using embeddings makes this less strict and makes it differentiable
		Ex. "country(Greece,Socrates)" matches rule "country(X,Y)"
		Ex. "located in" = "lies in"
		- Unification Score (some # between 0 and 1): the overall similarity between two logical atoms, calculated by combining the similarity scores of each individual component (ie. the predicates and arguments)
			- Similarity Scores : the cosine similarity is used to calculate the angle between vector representations of entities and predicates. (angle = similarity)
	- Backward-Chaining Theorem Prover:  Reasoners start with a conclusion and works backwards to see if there is data to support it. The process continues recursively, establishing sub-goals to be proven until it reaches facts that are known to be true
		- Process: 
			1. if the goal already stated in the knowledge base then it is proven
			2. If not, the program tries to unify the goal with the heads of every rule in the knowledge base
			3. For each rule if unification succeeds, every atom in the rule body becomes a subgoal
5. Pruning: Weak unification requires the prover to attempt unification between all entity and predicate symbols (making complexity exponential)
	1. Perform a lightweight evaluation of potential paths by comparing entities and predicates to see if they're similar enough for further analysis.
	2. Once a path passes the initial similarity threshold, NLProlog performs a more detailed analysis. The system tries to logically connect the entities and predicates along the path (ie. unification)
	3. When NLProlog finds a proof with a high success score it adjusts the similarity threshold to be the new score This means in future searches, NLProlog will ignore paths with initial scores lower than the best path
6. Results:
	- Proof: The program generates a proof of how it found the answer. This includes the sequence of rules that it used and the variable substitutions it used for every rule.
	- Proof Scores: The proof score is computed based on adding up the unification scores at each step of the proof. The highest score is the proof with the most concrete steps
	- Output: The highest scoring proof is outputted by the model
7. Neural Run-time: 
	- Models that are updated:
		1. The embedding matrix for encoding objects
		2. The MLP that finetunes SENT2VEC for encoding relations in facts
		3. The MLP that finetunes a concrete look up table for encoding relations in queries and rules
	- The encoders are trained to decide whether a given Prolog program entails the truth of a candidate triple. Once trained, the model will assign high probabilities to true triples, and low probabilities to false ones (using the rule set its been given)
		1. For a given candidate triple, the model generates all possible proofs based on the rules (above a certain proof score cutoff)
		2. For training, the model only uses the best proof it generates
		3. The difference between this proof score and the desired score (indicating correct reasoning) is the NN's loss function. Because the proof score is differentiable, the model can back-propagate the prediction error to update parameters 
8. Learning Rules Overtime:
	- Inductive Logic Programming: 
		- Case 1: User defines a set of rules with all relations empty. NLProlog then finds the most appropriate embeddings for the relations that make the rules true for the given examples by minimizing a loss function (w/ gradient descent)
			Ex. p1(X, Z) ⇐ p2(X, Y ) ∧ p3(Y, Z)
		- Case 2: User defines a set of rules and defines the relations in them. The model doesn't need to learn to fill these structures in but can use the facts for solving other problem (or filling other facts in)
			Ex. born_in(X, Z) ⇐ born_in(X, Y ) ∧ located_in(Y, Z)
 ![[Screenshot 2024-01-06 at 5.05.44 PM.png]]
### Testing
- Baselines
	1. Bidirectional Attention Flow: a technique that creates sophisticated mapping between the passage and question
	2. FastQA: for efficient and fast question answering 
	- Trained a separate model for each type of relation in WIKIHOP dataset
	- Converted their outputs into vector forms (and used similarity to pick best answer) in order to give them a fair chance
- Data Sets
	- MEDHOP: predict whether two drugs interact with each other, by considering the interactions between proteins that are mentioned in the support documents
		- Support Document Ex: "Aspirin interacts with Hemoglobin", "Hemoglobin affects efficacy of Metformin", "Aspirin w/ Metformin is bad"
- Format
	- Query: `born_in(Albert Einstein, X)`
	- Choices: `["Ulm, Germany", "Munich, Germany", "Berlin, Germany", "Vienna, Austria"]`
	- Answer: `"Ulm, Germany"`
- Process
	- Medhop: The embeddings for relations and objects are optimized. Pretrained embeddings specializing in biomed are used
	- Wikihop: The embeddings for just the relations are optimized (because there are too many objects in the dataset). Pretrained embeddings specializing in wikihop are used
	- Similarity Threshold = 0.5 (to start)
	-  Maximum Proof Depth = 3 (system can use just 3 logical rules to find a conclusion)
	- Rule Templates: direct relationship p(X,Y) ⇐ q(X,Y), inverse relationship p(X,Y) ⇐ q(Y,X), transitive relationship p(X, Z) ⇐ q(X, Y ) ∧ r(Y, Z)
- Results
	- Training with or without rules doesn't make too much of a difference
		- The model simply preforms unification without rules. It would be much more direct and less sophisticated, relying on the immediate similarity between entities and relations in triples
	- Much of the reasoning is simply performed by finding a suitable transformation of the object embeddings. The authors removed the MLP for converting (Add)
		Ex. to prove country(Oktabrskiy Big Concert Hall, Russia), model just makes the embeddings of Oktabrskiy Big Concert Hall and Saint Petersburg similar enough
![[Screenshot 2024-01-07 at 11.10.02 PM 1.png]]
### Personal Reflection
- Pros:
- Cons:
	- Weird that the model did so much worse when evaluated on a hidden test set. Embeddings were corrupted by question leakage maybe? (FastQA also did much worse though)
	- I was disappointed that there was no further discussion about interpretability (for me this feels like one of the biggest draws to neuro-symbolic reasoning)
	- Strange that rules and search didn't help all that much. It makes the paper feel less good to me
### Future Ideas
### Selfish Reading
### Other (Look at email)
### Terms to Memorize
- Predicate 



# Questions:
- "We describe how a Prolog reasoner can be enhanced with a differentiable unification function based on distributed representations (embeddings)" Why does making the unification function differentiable in this case matter? Are the embeddings using it for backpropogation or something? 
	- Why does the similarity function have to be differentiable? 
- The authors say their system is capable of "learning logical rules." How is that possible? 
- "Pseudo code for weak unification can be found in Appendix A – we refer the reader to (Russell and Norvig, 2010a) for an in-depth treatment of the unification procedure."
- The preprocessing step isn't actually first? Is it just a part of the Prolog Run-Time step
- "Inspired by fuzzy logic t- norms (Gupta and Qi, 1991), aggregation operators are e.g. the minimum or the product of all scores."
- "To transform the support documents to natural lan- guage triples, we first detect entities by perform- ing entity recognition with SPACY"
- Add back in? Unification: Determines whether a rule can be applied to a given situation by checking if the rule's conditions can be made to match the current state (ie. can the variables in 1 expression be substituted so that its equivalent to another expression)
			Ex. "country(Greece,Socrates)" matches the rule "country(X,Y)"
- How could this possibly train an encoder correctly? "The encoders are trained to decide whether a given Prolog program entails the truth of a candidate triple."
- Do I need to look further into how backpropogation works? the unification score is "end-to-end differentiable, and can be used for updating the model parameters θ via Stochastic Gradient Descent."
	- Do I need to understand stochastic vs normal grad descent
- Does this add up? "We solve this issue by using an external prover and pretrained sentence representations to efficiently discard all proof trees producing proof scores lower than a given threshold, significantly reducing the number of candidate proofs."
- How can the model learn embeddings for predicates (step 8) while concurrently learning the embeddings needed to encode data? 
- Look over innovation again and make it better 
- Put something in about proof depth
- Is this important: "Unless explicitly stated otherwise, all exper- iments were performed with the same set of rule templates containing two rules for each of the forms q(X,Y) ⇐ p2(X,Y), p1(X,Y) ⇐ p2(Y, X) and p1(X, Z) ⇐ p2(X, Y ) ∧ p3(Y, Z), where q is the query predicate"