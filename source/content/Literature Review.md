# Probabilistic Verification (General)
1. PAC (DeepPAC (Li et al., ICSE 2022))
    
2. Cross Coder Model Diffing
    
3. Formalizing the presumption of independence
    
4. Replication of Cross Coder Model Diffing
    
5. Estimating Tail Risk in Neural Networks: ([https://www.alignment.org/blog/low-probability-estimation-in-language-models/](https://www.alignment.org/blog/low-probability-estimation-in-language-models/))
    
6. Estimating the Probabilities of Rare Outputs in Language Models (https://arxiv.org/pdf/2410.13211)
    
7. Abstracting and Refining Provably Sufficient Explanations of Neural Network Predictions (https://openreview.net/pdf?id=1IeCqgULIM)
    
8. Weight Based Bilinear Decomposition ([https://arxiv.org/html/2406.03947v1#:~:text=Matrix%20Decompositions,55%2C%20b](https://arxiv.org/html/2406.03947v1#:~:text=Matrix%20Decompositions,55%2C%20b))
    
9. Local Interaction Basis
    
10. Sparse Autoencoders for Hallucination Mitigation (https://arxiv.org/html/2503.03032v1#:~:text=investigated,45)
    
11. Compact Proofs of Model Performance via Mechanistic Interpretability (https://arxiv.org/pdf/2406.11779)
    
12. Distributionally Robust Statistical Verification with Imprecise Neural Networks (https://arxiv.org/pdf/2308.14815)
    
13. Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control (https://arxiv.org/html/2405.08366v1)
    
14. SAEs are bad “Obfuscated Activations Bypass LLM Latent-Space Defenses” ([https://arxiv.org/pdf/2412.09565](https://arxiv.org/pdf/2412.09565))
    
15. INTERPRETING NEURAL NETWORKS THROUGH NEURON GROUPS (https://openreview.net/pdf?id=GdbQyFOUlJ)
    
16. On the Biology of a Large Language Model
    
17. Circuit Tracing: Revealing Computational Graphs in Language Models
    
18. KNOWLEDGE AWARENESS AND HALLUCINATIONS IN LANGUAGE MODELS
    
19. Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part
    
20. Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition
    
21. The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks (Not Scalable)
    
22. Scalable Quantitative Verification For Deep Neural Networks
    
23. SAVER: A Toolbox for Sampling-Based, Probabilistic Verification of Neural Networks
    
24. Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2
    
25. Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?
    
26. Can a Bayesian Oracle Prevent Harm from an Agent?
    
27. PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach
    
28. The Sweet Lesson: AI Safety Should Scale With Compute
    
29. (My understanding of) What Everyone in Technical Alignment is Doing and Why
    
30. Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI System
    
31. Deduction-Projection Estimators for Understanding Neural Networks
    
32. Forecasting Rare LLM Behaviors (just uses sampling and extrapolates from there)
    
33. An Analytic Solution to Covariance Propagation in Neural Networks (still adds some error when propagating covariances but we can worst-case the error) (issue is that you can pass in a gaussian but it becomes not gaussian after the first transformation function)
    
34. DISCOVERING LATENT KNOWLEDGE IN LANGUAGE MODELS WITHOUT SUPERVISION
    
35. How "Discovering Latent Knowledge in Language Models Without Supervision" Fits Into a Broader Alignment Scheme
    
36. The Alignment Trap: Complexity Barriers
    
37. Probabilistic Verification of ReLU Neural Networks via Characteristic Functions
    
38. Calibrating Transformers via Sparse Gaussian Processes (talks about Kernel Density Estimation)
    
39. Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach (shows that white-box methods perform best!! But is basically just reading the tea leaves)
    
# Probabilistic Verification (Using Counterexamples)

1. A STATISTICAL APPROACH TO ASSESSING NEURAL NETWORK ROBUSTNESS
    
2. ESTIMATING THE PROBABILITIES OF RARE OUTPUTS IN LANGUAGE MODELS
    
3. Towards Robust LLMs: an Adversarial Robustness Measurement Framework
    
4. Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs
    
5. A Probabilistic Perspective on Unlearning and Alignment for Large Language Models 
    
6. Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo (reweight the OUTPUT distribution instead of the input distribution) 
    

# Formal Verification 

1. Bilinear MLPs enable weight-based mechanistic interpretability (https://arxiv.org/html/2410.08417v1)
    
2. Attribution-based Parameter Decomposition ([https://arxiv.org/pdf/2501.14926](https://arxiv.org/pdf/2501.14926))
    
3. Distributionally Robust Statistical Verification with Imprecise Neural Networks
    
4. TOWARDS RELIABLE NEURAL SPECIFICATIONS
    
5. Learning Minimal Neural Specifications
    
6. Out of the Shadows: Exploring a Latent Space for Neural Network Verification
    
7. Training Verification-Friendly Neural Networks via Neuron Behavior Consistency
    
8. Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control
    
9. Compact Proofs of Model Performance via Mechanistic Interpretability
    
10. Robustness Verification for Transformers
    
11. Training Neural Networks for Modularity Aids Interpretability
    
# Past Papers (Formal Verification)

1. Critically Assessing the State of the Art in Neural Network Verification
    
2. Verification of Neural Networks' Global Robustness
    
3. Certifying Global Robustness for Deep Neural Networks
    
4. Multi-Hour Blood Glucose Prediction in Type 1 Diabetes: A Patient-Specific Approach Using Shallow Neural Network Models
    
5. Conformance Verification for Neural Network Models of Glucose-Insulin Dynamics
    
6. Towards Reliable Neural Specifications
    
7. Learning Minimal Neural Specifications
    
8. Formal Verification Techniques for Vision-Based Autonomous Systems – A Survey
    
9. Detecting hallucinations in large language models using semantic entropy
    
10. A Survey on Uncertainty Quantification Methods for Deep Learning
    
11. Probabilistic Verification of Neural Networks using Branch and Bound
    
12. PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach
    
13. Towards Scalable Complete Verification of ReLU Neural Networks via Dependency-based Branching
    
14. INNAbstract: An INN-Based Abstraction Method for Large-Scale Neural Network Verification
    
15. The Mathematics of Kolmogorov-Arnold-Networks versus Artificial Neural Networks
    
16. First three years of the international verification of neural networks competition (VNN-COMP)
    
17. Reachability Analysis for Cyber-Physical Systems: Are we there yet?
    
18. The Marabou Framework for Verification and Analysis of Deep Neural Networks
    
19. Introduction to Neural Network Verification
    
20. Training and Verifying robust Kolmogorov-Arnold Networks
    