Core of all my beliefs: (10) Race dynamics mean first company to AGI will have cut corners
- (10) Axiom: There is nothing inherently special about being a human, at some point we'll make an intelligence comparable to ours
- (9.5) Axiom: It's unclear how much what we learn from small systems will scale to large systems
	- (8.5) We will probably only get a single try to turn on a super intelligent system (ie. 1 try to get alignment right)
- (7.5) Axiom: We need to solve the alignment problem in order for the super intelligent system we make to be aligned. There are too many possible goals and too much variability to assume otherwise
# The Specifics
- (8.7) Assumption: Neural nets can optimize for nearly any situation (using backpropogation). This will generalize far enough and scale large enough to give us AGI without any big changes to the architecture.
- (9.5) Assumption: Compute is on track to be 30 times what it currently is in 10 years
- Assumption: the ability to find features will scale with neural network 
# Eliezer's Take
###### Why It's Dangerous 
*That we have to get a bunch of key stuff right on the first try is where most of the lethality really and ultimately comes from*
1. Orthogonality: there can exist arbitrarily intelligent agents pursuing any kind of goal. (even a superintelligent AI would be willing to make paperclips for the rest of its life)
2. Instrumental Convergence: We can take a lot specific goals and find very similar optimal strategies to solve them (like how you need to harness electricity to manufacture both paperclips and cars)
	- This means: The best and easiest-found-by-optimization algorithms for solving problems we want an AI to solve, readily generalize to problems we'd rather the AI not solve
3. Usually science gets 100 years to solve a problem with unlimited retries. This time we'll get only 1 try and much less time
4. AGI will not be upper-bounded by human ability (ie. the way AlphaGo blew past humans)
5. An aligned AI will need to perform a huge task to make sure no other systems rise to power and become dangerous (ie. burn all GPUs). This means that the AI must be powerful enough to be dangerous itself
6. Even if an AI is aligned, it must be always maintained to keep alignment (the equivalent of nuclear cores that require actively maintained design properties to not go supercritical and melt down)
###### Why Alignment is so Hard
*But can't you just use a loss function of "do what humans want" and then everything will be alright?
1. You can't train alignment by running lethally dangerous cognitions, observing whether the outputs kill or deceive or corrupt the operators, assigning a loss, and doing supervised learning.
	- This means you need to hope that the alignment generalizes from a safe system to a much more intelligent one
	- In order to carry out the huge task to stop other AGI's from being created, you need to have an AGI that's been aligned on much smaller tasks and hope that its alignment transfers.
		- You won't be able to verify that the huge task the AI plans to carry out is good because it's too complicated for us
2. Problems that materialize at high intelligence and danger levels may fail to show up at safe lower levels of intelligence.
	- You can't know all of the problems that will appear when a model gets to above an intelligence threshold
		Ex. The model will act completely differently when it knows it can defeat its creators than when it doesn't
3. Even if you train really hard on a loss function, you might not create an internal representation of the loss function inside the AI. When the AI gets much smarter it's unlikely to follow the loss function
	- The first solutions found that seem aligned on the outside are very likely to not be inner-aligned solutions
	- Currently, we don't know how to get inner properties into a model (or verify that they're there)
	- If you show an agent a reward signal that's generated by humans, the signal doesn't say _how aligned the action was_, because another way of producing a high reward is to deceive, corrupt, or replace the humans
		- Eventually, the AI will learn that it's much more effective to predict the process generating its reward function than to try to follow the reward function exactly 
	- If reward signal not generated by humans then lethal-to-us possibilities exist in some possible environments underlying every given sense input (every world that looks good from a webcam may not be safe for us creatures outside the webcam)
4. Capabilities generalize further than alignment once capabilities start to generalize far (humans are optimized to pass their genes onto the next generation but when we became smarter and smarter we generalized the capabilities we had gotten to get to the moon)
	- Outer Optimizer = inclusive genetic fitness, Inner Optimizer = human ingenuity
5. How do you make an AI with a goal but also one that allows itself to be shutoff (ie. you can't bring the coffee if you're dead)
	- The smarter the AI gets, the worse this problem gets
###### Why Interpretability is so Hard
*Drawing interesting graphs of where a transformer layer is focusing attention doesn't help if the question that needs answering is "So was it planning how to kill us or not?"*
1. Knowing that a medium-strength system is planning to kill us, does not et us build a high-strength system that isn't planning to kill us
2. When you try to optimize a model against unaligned thoughts, you just get a model better at hiding those thoughts
3. Even if there were consensus about a risk from powerful AI systems, there is a good chance that the world would respond in a totally unproductive way. Humanity is fully capable of messing up even very basic challenges
4. Alignment isn't a typical research problem, it was picked because its important not because humans can make progress on it easily

# Paul's Response
*Main Crux: By the time AI systems can double the pace of AI research (recursive self-improvement), it seems like they can greatly accelerate the pace of alignment research*
###### Agreements
1. People’s minds seem to go to what they think of as “more realistic and less sci fi” even though that's not logical
2. AI systems will go from “large impact on the world” to “unrecognizably transformed world” much quicker than most expect 
3. Reality won’t “force us” to solve alignment until it’s too late (because it's so easy to build products with AI that's not aligned)
###### Disagreements
 1. We will be able to learn a lot about alignment from experiments and trial and error. We're not necessarily doomed just because we only get 1 try to get things right: we may still get plenty of data to learn from 
 2. Capabilities will likely be continuously increasing because gradient descent is always selecting for the best model. Ie. a system won't be able to hide it's abilities and surprise us (this doesn't account for recursive self-improvement)
 3. AI improving itself will probably just be AI doing research and development like a human would
	 - AI might be able to have extremely fast recursive self improvement at some point, but it would first need to reach superhuman levels in other areas like alignment research
	 - This might debunk the theory that we need to be super worried about recursive self-improvement since AI is accelerating alignment and AI capabilities simultaneously (the claim of very fast take off speeds is a wild guess, not rooted in much evidence)
 4. An aligned AI doesn't need to preform a pivotal act at all: it can simply advance alignment research enough or demonstrate why unaligned intelligences are so dangerous
 5. In general, most of Yudkowsky's arguments are more inferences than observations even though he presents them as fact (ie. how AI will generalize outside of distribution, how AI will converge to particular goals...)
	 - For instance, there's no evidence that alignment will need to deal with a huge distributional shift from low to very high intelligence. This is because AI training is continuous and gradually increases intelligence
	 - He underestimates how much better we'll be at alignment in the future vs now
6. Humanity has put in a very low amount of effort into solving alignment so it's probably not as doomed as yudkowsky thinks 
7. Natural Selection is a weak analogy for ML training because we're able to shape models we train. Animal breeding is better and here it seems like alignment works (turning wolves into dogs)
8. 
# Other's Response
*None of the lethal concerns are addressed, the response below is only saying we may have a bit more time to prepare
1. Eliezer overplays the role of intelligence. Humans took over apes because they could coordinate. One human can't kill a whole tribe of apes, one AGI can't kill a whole world of coordinating humans
2. Getting low-hanging fruit is so so much easier. You can't use Alphago as an example because Google just tackled the low-hanging fruit of go
	- This means we'll have more time to prep, an AI can't just jump from 50th percentile to 99th percentile in a day
# Possible Solutions
1. RL is performed on a reward function that gets smarter in parallel with the agent being trained
2. Iterated Distillation and Amplification: start with a small aligned model. Amplify it (ie. use the alpha go strategy) to make a better agent (that we hope is aligned) and then distill it (use it to train the smaller system to be better). Use the new smaller system for again, continue the cycle
3. using interpretability tools for training
# Questions
1. "We have standards for interpretability that we can’t yet meet" look into this 
2. "We have toy models of alignment failures" look into this 
3. Do I think that a model would recursively self-improve very quickly (ie. just rewrite the software to be better gives instant intelligence increase) or do I think that the model would need to follow the more traditional Research and Development process of a normal AI researcher?
	- Does this even matter for Paul's argument?
4. Could I start research into looking at how AI generalizes outside of training distribution. Is this related to how deep RLHF goes in a model?
5. 


![[Screenshot 2024-05-16 at 7.05.52 PM.png]]